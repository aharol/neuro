{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- : utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue May 29 21:12:40 2018\n",
    "\n",
    "@author: hielke & niels\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import random\n",
    "from pprint import PrettyPrinter as pp\n",
    "from matplotlib import pyplot as plt\n",
    "pprint = pp().pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Hopfield networks are a type of autoencoder, which are able to reconstruct a complete dataset while only a limited amount of data is available. To achieve this, the network must first be trained on the undistorted data. \n",
    "In our code, we trained the networks as described below in \"hebbian\". \n",
    "A trained network will have one or multiple stable states. A random input state will converge to one of these states when it is repeatedly updated. Note that states must be binary to guarantee convergence. In this case neurons could be in the state -1 or 1.\n",
    "\n",
    "For each to be learned configuration, a hebbian association matrix is constructed where matrix[i, j] is calculated by the product of the states of neuron i and j. This implies that a Hopfield network is symmetric: product i,j is equal to product j,i. Thus  weights[i, j] is equal to weights[j, i]. Furthermore, the elements for which i=j (i.e. the diagonal of the matrix) will be zero because a Hopfield network does not contain self-loops. Once such a matrix is constructed for each of the to be learned states, all matrices are added to make the weights matrix. This matrix is implemented in \"hebbian\" and \"multiple_hebbian\" below. \n",
    "\n",
    "The update procedure relies on energy minimalization. Energy is determined by the weights matrix. If changing the state of a neuron to 1 will decrease the total energy of the system, the state will be adapted. Otherwise, it will be -1. The change in energy is calculated as follows and is implemented in \"update\". The association of each neuron with the evaluated neuron j (i.e. weights[i, j] for i in range(number of neurons)) is multiplied by the state of neuron j. If the sum of these values is larger than the threshold associated with this neuron, the state will be set to 1.\n",
    "\n",
    "In our code, the update rule is applied until convergence or until the system has been updated 1000 times. In the first case, the network successfully reconstructed one of the stable states; in the second case, it failed to do so.\n",
    "Below, we describe our class \"Hopfield\" used to implement a Hopfield network.\n",
    "\n",
    "# class Hopfield\n",
    "## hebbian\n",
    "This method constructs the hebbian association matrix (called \"weights\") given a vector that contains the state of each neuron:\n",
    "First, the transpose of the vector is multiplied with the vector itself resulting in a matrix. Since a Hopfield network requires that a neuron is not directly connected to itself, the diagonal must be zero. \n",
    "Because the states are always either 1 or -1, the product of a neuron state with itself is always 1. Therefore, the diagonal can be reduced to zero by subtracting the identity matrix from the previously calculated matrix. \n",
    "\n",
    "## multiple_hebbian\n",
    "Similar to hebbian described above, multiple_hebbian constructs the weights matrix. The difference is that this method takes a matrix of inputs (multiple, unique states) instead of a vector (a single state). \n",
    "\n",
    "## reset_states\n",
    "Resets the states: randomizes the state of each neuron to either -1 or 1. So that different trials will be independent of each other.\n",
    "\n",
    "\n",
    "## update\n",
    "For every update, a random neuron is chosen with random.choice. If the sum of the weights of each connection with this neuron is larger than its respective threshold, the state will be set to 1. Otherwise it will be set to -1. \n",
    "\n",
    "\n",
    "## converge\n",
    "This function updates the system until it converges or until a specified number of iterations has been concluded. After every update, the change of the updated neuron i is tracked. If its state is the same as before, changed_state[i] will be set to False. If all elements of changed_state are set to False, the system has converged. On the other hand, if the maximum number of iterations has been met, the system has not converged.\n",
    "\n",
    "## list_attractors\n",
    "This function keeps track of all attractor states (i.e. the states the system can converge to). The converge function is used 100 times independently. For each of these tries, the neurons are independently randomized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hopfield:\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        weights = np.asarray(weights)\n",
    "        size = len(weights)\n",
    "        assert size * size == np.size(weights), \\\n",
    "                    \"Weights should be square matrix\"\n",
    "\n",
    "        self.weights = weights\n",
    "        self.size = size\n",
    "        \n",
    "        self.reset_states()\n",
    "        self.tresholds = np.zeros(size) \n",
    "        \n",
    "        assert not (self.weights * np.eye(self.size)).all(), \\\n",
    "                \"There are self-loops\"\n",
    "        assert np.allclose(self.weights, self.weights.T), \\\n",
    "                \"Not symmetric\"\n",
    "        \n",
    "    @classmethod\n",
    "    def hebbian(cls, vector):\n",
    "        size = np.size(vector)\n",
    "        vector = np.asmatrix(vector)\n",
    "        weights = vector.T * vector - np.eye(size)\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def multiple_hebbian(cls, matrix):\n",
    "        size = np.size(matrix[0, :])\n",
    "        amm_vectors = len(matrix)\n",
    "        assert np.size(matrix) == size * amm_vectors, \\\n",
    "                \"Not all vectors are of the same length\"\n",
    "        weights = np.matlib.zeros((size, size))\n",
    "        for i in range(amm_vectors):\n",
    "            vec = np.asmatrix(matrix[i, :])\n",
    "            weights += vec.T * vec\n",
    "        weights -= amm_vectors * np.matlib.eye(size)\n",
    "        return cls(weights)\n",
    "    \n",
    "    @classmethod\n",
    "    def with_tresh(cls, weights, tresh):\n",
    "        hp = cls(weights)\n",
    "        hp.tresholds = tresh\n",
    "        return hp\n",
    "        \n",
    "    def reset_states(self):\n",
    "        self.states = np.array([random.choice([-1, 1]) \n",
    "                for _ in range(self.size)], dtype=int)\n",
    "    \n",
    "    def update(self):\n",
    "        i = random.choice(range(self.size))\n",
    "        \n",
    "        delta = 0\n",
    "        for j in range(self.size):\n",
    "            delta += self.weights[i, j] * self.states[j]\n",
    "        \n",
    "        new_state = 1 if delta >= self.tresholds[i] else -1\n",
    "        \n",
    "        changed = self.states[i] != new_state\n",
    "        self.states[i] = new_state\n",
    "        \n",
    "        return i, changed\n",
    "        \n",
    "    def converge(self):\n",
    "        changed_states = [True for _ in range(self.size)]\n",
    "        counter = 0\n",
    "        while any(changed_states):\n",
    "            i, change = self.update()\n",
    "            if change:\n",
    "                changed_states = [True for _ in range(self.size)]\n",
    "            else:\n",
    "                changed_states[i] = False\n",
    "            counter += 1\n",
    "            if counter > 10000:\n",
    "                return print(\"ERR: exceeded maximum iterations.\")\n",
    "        return self.states\n",
    "    \n",
    "    def list_attractors(self):\n",
    "        attr_list = []\n",
    "        \n",
    "        for _ in range(100):\n",
    "            self.reset_states()\n",
    "            attr = self.converge()\n",
    "            \n",
    "            for a in attr_list:\n",
    "                if np.array_equal(a, attr):\n",
    "                    break\n",
    "            else:\n",
    "                attr_list.append(attr)\n",
    "        \n",
    "        return attr_list\n",
    "    \n",
    "    def plot_neuron_states(self, height, width, states=None):\n",
    "        if states is None:\n",
    "            states = self.states\n",
    "        print(height, width, np.size(states))\n",
    "        assert height * width == np.size(states), \\\n",
    "                \"Height and width do not match the size of the states.\"\n",
    "        \n",
    "        image = []\n",
    "        \n",
    "        for h in range(height):\n",
    "            image.append(states[h*width:(h+1)*width])\n",
    "        plt.figure()\n",
    "        plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple convergence\n",
    "This function takes a trivial matrix of which the diagonals are 0 (thus implying that there are no self-connections) and feeds it to the Hopfield class. Consequently, the class method  \"converge\" is applied in the outer loop of a nested for loop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attractors:\n",
      "[array([-1,  1, -1]), array([ 1, -1,  1])]\n"
     ]
    }
   ],
   "source": [
    "weights = np.matrix([[0, -1, 1],\n",
    "                    [0, 0, -1],\n",
    "                    [0, 0, 0]])\n",
    "weights += weights.T\n",
    "hp = Hopfield(weights)\n",
    "print(\"attractors:\")\n",
    "pprint(hp.list_attractors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Hebbian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attractors:\n",
      "[array([1, 1, 1, 1, 1, 1, 1, 1]), array([-1, -1, -1, -1, -1, -1, -1, -1])]\n"
     ]
    }
   ],
   "source": [
    "vector = np.matrix([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "hp = Hopfield.hebbian(vector)\n",
    "print(\"attractors:\")\n",
    "pprint(hp.list_attractors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again a simple Hebbian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attractors:\n",
      "[array([ 1,  1, -1,  1]), array([-1, -1,  1, -1])]\n"
     ]
    }
   ],
   "source": [
    "vector = np.matrix([1,1,-1,1])\n",
    "hp = Hopfield.hebbian(vector)\n",
    "print(\"attractors:\")\n",
    "pprint(hp.list_attractors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example with multiple things to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attractors:\n",
      "[array([ 1, -1,  1, -1]),\n",
      " array([-1,  1, -1,  1]),\n",
      " array([ 1,  1, -1, -1]),\n",
      " array([-1, -1, -1, -1]),\n",
      " array([-1, -1,  1,  1]),\n",
      " array([1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 1, 1, 1],\n",
    "                   [1, -1, 1, -1],\n",
    "                           [1, 1, -1, -1]])\n",
    "hp = Hopfield.multiple_hebbian(matrix)\n",
    "print(\"attractors:\")\n",
    "pprint(hp.list_attractors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron learning\n",
    "\n",
    "In addition to Hebbian learning you can also transform your weights and desired output in a certain way to feed it to a perceptron learner. This is the tranformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(vector):\n",
    "    size = len(vector)\n",
    "    tres = np.ones((1,size))\n",
    "    z = np.zeros((size, size + size*(size-1)//2))\n",
    "    for i in range(size):\n",
    "        ran = (size-1)*size//2 - (size-1-i)*(size-i)//2\n",
    "        z[i,ran:ran+size-i-1] = vector[i+1:]\n",
    "        z[i,-(size-i)] = tres[0,i]\n",
    "        for j in range(i):\n",
    "            z[i, ]\n",
    "        j = i\n",
    "        while j > 0:\n",
    "            ran = (size-1)*size//2 -(size-j)*(size-j+1)//2\n",
    "            z[i, i-1+ran] = vector[j-1]\n",
    "            j -= 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the function to learn the perceptron. It takes a matrix with the desired outputs as rows, and the outputs the weights and the tresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopfield_perceptron(matrix):\n",
    "    \n",
    "    rate = .01 # The learning rate, increasing can cause faster convergence, \n",
    "               # but can also cause oscillations around a stable state.\n",
    "    err = .4   # The error rate, increasing it, can stabilize the network more,\n",
    "               # but increasing it too much can again cause oscillations around a stable state.\n",
    "    \n",
    "    vector = matrix[0, :]\n",
    "    z = transformation(vector)\n",
    "    sol = np.array([random.choice([-.01, .01]) for _ in range(len(z[0,:]))])\n",
    "    \n",
    "    amm_vectors = len(matrix[:, 0])\n",
    "    \n",
    "    for i in range(1000):\n",
    "        still_wrong = False\n",
    "        for j in range(amm_vectors):\n",
    "            vector = matrix[j, :]\n",
    "            z = transformation(vector)\n",
    "            for i in range(len(vector)):\n",
    "                if not sol.dot(z[i, :]) * vector[i] > err: # prediction incorrect\n",
    "                    sol += rate * z[i, :] * vector[i]\n",
    "                    still_wrong = True\n",
    "        if not still_wrong:\n",
    "            print(\"We are good\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"We have not converged\")\n",
    "        \n",
    "    triu = zip(*np.triu_indices(len(vector), 1))\n",
    "    weights = np.matlib.zeros((len(vector), len(vector)))\n",
    "    for ind, iu in enumerate(triu):\n",
    "        weights[iu] = sol[ind]\n",
    "    \n",
    "    weights += weights.T\n",
    "\n",
    "    tresh = -1 * sol[-len(vector):]\n",
    "    \n",
    "    return weights, tresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are good\n",
      "[array([ 1,  1, -1,  1, -1,  1, -1, -1]),\n",
      " array([ 1, -1,  1, -1,  1, -1,  1,  1])]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 1, -1, 1, -1, 1, -1, -1],\n",
    "                       [1, -1, 1, -1, 1, -1, 1, 1]])\n",
    "weights, tresh = hopfield_perceptron(matrix)\n",
    "hp = Hopfield.with_tresh(weights, tresh)\n",
    "attractors = hp.list_attractors()\n",
    "pprint(attractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example with too much too learn\n",
    "\n",
    "In this example the network must learn too much, and will therefore have incorrect attractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have not converged\n",
      "[array([-1, -1, -1, -1,  1,  1,  1,  1]),\n",
      " array([ 1,  1,  1,  1, -1, -1, -1, -1]),\n",
      " array([ 1, -1,  1, -1,  1, -1,  1,  1]),\n",
      " array([ 1, -1, -1, -1, -1,  1,  1,  1])]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 1, -1, 1, -1, 1, -1, -1],\n",
    "                       [1, 1, -1, 1, -1, -1, -1, 1],\n",
    "                       [1, 1, 1, 1, -1, -1, -1, -1],\n",
    "                       [1, -1, 1, -1, 1, -1, 1, 1]])\n",
    "weights, tresh = hopfield_perceptron(matrix)\n",
    "hp = Hopfield.with_tresh(weights, tresh)\n",
    "attractors = hp.list_attractors()\n",
    "pprint(attractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same problem with Hebbian learning\n",
    "\n",
    "Interestingly, if we feed this same problem into the hebbian learning algorithm, it will find less attractors than the perceptron. So, the perceptron is closer to the solution, but still cannot find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attractors:\n",
      "[array([-1, -1,  1, -1,  1,  1,  1,  1]),\n",
      " array([ 1,  1, -1,  1, -1,  1, -1,  1])]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 1, -1, 1, -1, 1, -1, -1],\n",
    "                   [1, 1, -1, 1, -1, -1, -1, 1],\n",
    "                   [1, 1, 1, 1, -1, -1, -1, -1],\n",
    "                   [-1, 1, -1, 1, -1, 1, -1, 1]])\n",
    "hp = Hopfield.multiple_hebbian(matrix)\n",
    "print(\"attractors:\")\n",
    "attractors = hp.list_attractors()\n",
    "pprint(attractors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
